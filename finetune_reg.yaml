# Config file for finetuning regression model (task 3 in FOMO25 challenge)
# Global settings; don't add new--manually passed
global_settings:
  seed: 12345
  project: CLwAux-downstream-task3
  experiment: allin_epoch95_noreg_100
  root_dir: /projects/prjs1526/anyBrainer/experiments

# Logging settings; add new entries only in dedicated sections or as extra_kwargs
logging_settings:
  dev_mode: False
  worker_logs: False
  save_logs: True
  wandb_enable: True
  wandb_watch_enable: True
  wandb_watch_kwargs: # can add
    log_freq: 100
    log_graph: True
    log: 'all'

# Data settings; add new entries only in dedicated sections or as extra_kwargs
pl_datamodule_settings:
  name: ClassificationDataModule
  data_dir: /projects/prjs1526/FOMO-MRI/fomo-task3_clean
  data_handler_kwargs: # can add
    data_format: GenericNifti
    exts: [".nii.gz"]
  train_val_test_split: [0.95, 0.05, 0.0]
  num_workers: 4
  batch_size: 2
  extra_dataloader_kwargs: # can add
    shuffle: True
    pin_memory: False
  train_transforms:
    name: get_regression_train_transforms
    keys: ["t1", "t2"]
    allow_missing_keys: False
    is_nifti: True
    concat_img: True
    sliding_window: True
    target_key: "img"
  val_transforms:
    name: get_downstream_val_transforms
    keys: ["t1", "t2"]
    allow_missing_keys: False
    is_nifti: True
    concat_img: True
    sliding_window: True
    target_key: "img"
  test_transforms: 
    name: get_downstream_val_transforms
    keys: ["t1", "t2"]
    allow_missing_keys: False
    is_nifti: True
    concat_img: True
    sliding_window: True
    target_key: "img"
  predict_transforms: 
    name: get_predict_transforms
    keys: ["t1", "t2"]
    allow_missing_keys: False
    is_nifti: True
    concat_img: True
    sliding_window: True
    target_key: "img"
  extra_kwargs:
    labels_dir: /projects/prjs1526/FOMO-FTDS/fomo-task3/labels
    expected_labels: [0, 100]

# Model settings; apart from name, automatically passed as kwargs
pl_module_settings:
  name: RegressionModel
  model_kwargs:
    name: Swinv2Classifier
    patch_size: 2
    depths: [2, 2, 6, 2]
    num_heads: [3, 6, 12, 24]
    window_size: 7
    feature_size: 48
    use_v2: true
    extra_swin_kwargs:
      use_checkpoint: True
    mlp_num_classes: 1
    mlp_num_hidden_layers: 1
    mlp_hidden_dim: 128
    mlp_dropout: 0.2
    mlp_activations: GELU
    late_fusion: True
    n_late_fusion: 2
  optimizer_kwargs:
    name: AdamW
    auto_no_weight_decay: True
    param_groups:
      - lr: 0.0005
        weight_decay: 0.001
        param_group_prefix: [fusion_head, classification_head]
      - lr: 0.0001
        weight_decay: 0.00005
        param_group_prefix: encoder.layers4
      - lr: 0.00007
        weight_decay: 0.00005
        param_group_prefix: encoder.layers3
      - lr: 0.00005
        weight_decay: 0.00005
        param_group_prefix: encoder.layers2
      - lr: 0.00003
        weight_decay: 0.00005
        param_group_prefix: encoder.layers1
  lr_scheduler_kwargs:
    name: CosineAnnealingWithWarmup
    interval: step
    frequency: 1
    eta_min: [0.00005, 0.00001, 0.000007, 0.000005, 0.000003]
    warmup_iters: [192, 232, 252, 216, 160] # [8%, 12%, 15%, 18%, 22%]
    start_iter: [0, 480, 720, 1200, 1680]
    total_iters: 2400
  loss_fn_kwargs:
    name: SmoothL1Loss
    beta: 0.1
  weights_init_settings:
    weights_init_fn: init_swin_v2
    load_pretrain_weights: /projects/prjs1526/anyBrainer/experiments/CLwAux-baseline/full/checkpoints/epoch=94.ckpt
    load_param_group_prefix: model.encoder
    rename_map:
        model.encoder: encoder
  center_labels: "fixed:55"
  scale_labels: [20, 90]
  bias_init:

# Checkpoint settings; add new entries only in dedicated sections
ckpt_settings:
  new_version: True
  model_checkpoint:
  save_every_n_epochs: 10
  save_top_k: 1
  save_last: False
  extra_ckpt_kwargs: # add any extra checkpoint kwargs here
    save_on_train_epoch_end: True

# Callback settings (other than model checkpointing); automatically passed as kwargs
pl_callback_settings:
  - name: FreezeParamGroups
    param_group_prefix: [encoder.layers4, encoder.layers3, encoder.layers2, encoder.layers1, encoder.patch_embed]
    freeze_epoch: 0
    unfreeze_epoch: [20, 30, 50, 70, -1]

# Trainer settings; except for logger, callbacks, reload_dataloaders_every_n_epochs, automatically passed as kwargs
pl_trainer_settings:
  max_epochs: 100
  accumulate_grad_batches: 4
  gradient_clip_val: 3.0
  gradient_clip_algorithm: norm
  precision: bf16-mixed
  strategy: auto
  devices: auto
  num_nodes: 1
  log_every_n_steps: 5
  val_check_interval: 1.0
  check_val_every_n_epoch: 1
  num_sanity_val_steps: 2
  overfit_batches: 0.0
  fast_dev_run: false
  limit_train_batches: 1.0
  limit_val_batches: 1.0
  limit_test_batches: 1.0
  limit_predict_batches: 1.0
  accelerator: auto

# Validation settings; add new entries only in dedicated sections
val_settings:
  val_mode: repeated
  n_splits: 5
  start_idx: 0
  run_test: False
  seeds: [12345, 12346, 12347, 12348, 12349]
  aggregate_metrics: ['val/', 'test/']
