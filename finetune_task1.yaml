# Global settings; don't add new--manually passed
global_settings:
  seed: 12345
  project: CLwAux-downstream-task1
  experiment: allin_epoch95_100_patch
  root_dir: /projects/prjs1526/anyBrainer/experiments

# Logging settings; add new entries only in dedicated sections or as extra_kwargs
logging_settings:
  dev_mode: False
  worker_logs: False
  save_logs: True
  wandb_enable: True
  wandb_watch_enable: True
  wandb_watch_kwargs: # can add
    log_freq: 40
    log_graph: True
    log: 'all'

# Data settings; add new entries only in dedicated sections or as extra_kwargs
pl_datamodule_settings:
  name: ClassificationDataModule
  data_dir: /projects/prjs1526/FOMO-MRI/fomo-task1_clean
  data_handler_kwargs: # can add
    data_format: GenericNifti
    exts: [".nii.gz"]
  train_val_test_split: [1.0, 0.0, 0.0]
  num_workers: 4
  batch_size: 1
  extra_dataloader_kwargs: # can add
    shuffle: True
    pin_memory: False
  train_transforms:
    name: get_classification_train_transforms
    keys: ["flair", "dwi", "adc", "swi", "t2s"]
    allow_missing_keys: True
    is_nifti: True
    concat_img: True
    sliding_window: False
    target_key: "img"
  val_transforms:
    name: get_downstream_val_transforms
    keys: ["flair", "dwi", "adc", "swi", "t2s"]
    allow_missing_keys: True
    is_nifti: True
    concat_img: True
    sliding_window: False
    target_key: "img"
  test_transforms: 
    name: get_downstream_val_transforms
    keys: ["flair", "dwi", "adc", "swi", "t2s"]
    allow_missing_keys: True
    is_nifti: True
    concat_img: True
    sliding_window: False
    target_key: "img"
  predict_transforms: 
    name: get_predict_transforms
    keys: ["flair", "dwi", "adc", "swi", "t2s"]
    allow_missing_keys: True
    is_nifti: True
    concat_img: True
    sliding_window: False
    target_key: "img"
  extra_kwargs:
    labels_dir: /projects/prjs1526/FOMO-FTDS/fomo-task1/labels
    modalities: ["flair", "dwi", "adc", "swi", "t2s"]
    get_seg_masks: False
    seg_filename: "seg.nii.gz"

# Model settings; apart from name, automatically passed as kwargs
pl_module_settings:
  name: ClassificationModel
  model_kwargs:
    name: Swinv2Classifier
    patch_size: 2
    depths: [2, 2, 6, 2]
    num_heads: [3, 6, 12, 24]
    window_size: 7
    feature_size: 48
    use_v2: true
    extra_swin_kwargs:
      use_checkpoint: True
    mlp_num_classes: 1
    mlp_num_hidden_layers: 1
    mlp_hidden_dim: 128
    mlp_dropout: 0.3
    mlp_activations: GELU
    expect_patch_dim: False
    late_fusion: True
    n_late_fusion: 4
  optimizer_kwargs:
    name: AdamW
    auto_no_weight_decay: True
    param_groups:
      - lr: 0.0005
        weight_decay: 0.001
        param_group_prefix: [classification_head, fusion_head]
      - lr: 0.00002
        weight_decay: 0.00005
        param_group_prefix: [encoder.layers4]
  lr_scheduler_kwargs:
    name: CosineAnnealingWithWarmup
    interval: step
    frequency: 1
    warmup_iters: [30, 30] # [10%, 16.7%]
    start_iter: [0, 420]
    eta_min: [0.00005, 0.000002]
    total_iters: 600
  loss_fn_kwargs:
    name: BCEWithLogitsLoss
  weights_init_settings:
    weights_init_fn: init_swin_v2
    load_pretrain_weights: /projects/prjs1526/anyBrainer/experiments/CLwAux-baseline/full/checkpoints/epoch=94.ckpt
    load_param_group_prefix: model.encoder
    rename_map:
      model.encoder: "encoder"
  inference_settings:
      inferer_kwargs:
        name: SlidingWindowClassificationInferer
        patch_size: 128
        n_patches: [2, 2, 2]
        aggregation_mode: "mean"
      postprocess: get_postprocess_classification_transforms
      tta: get_flip_tta

# Checkpoint settings; add new entries only in dedicated sections
ckpt_settings:
  new_version: True
  model_checkpoint:
  save_every_n_epochs: 20
  save_top_k: 1
  save_last: False
  extra_ckpt_kwargs: # add any extra checkpoint kwargs here
    save_on_train_epoch_end: True

# Callback settings (other than model checkpointing); automatically passed as kwargs
pl_callback_settings:
  - name: FreezeParamGroups
    param_group_prefix: [encoder.layers4, encoder.layers3, encoder.layers2, encoder.layers1, encoder.patch_embed]
    freeze_epoch: 0
    unfreeze_epoch: [70, -1, -1, -1, -1]
  - name: SWAAvgOnly
    start_epoch: 80
    update_on: epoch

# Trainer settings; except for logger, callbacks, reload_dataloaders_every_n_epochs, automatically passed as kwargs
pl_trainer_settings:
  max_epochs: 100
  accumulate_grad_batches: 4
  gradient_clip_val: 3.0
  gradient_clip_algorithm: norm
  precision: bf16-mixed
  strategy: auto
  devices: auto
  num_nodes: 1
  log_every_n_steps: 1
  val_check_interval: 1.0
  check_val_every_n_epoch: 5
  num_sanity_val_steps: 2
  overfit_batches: 0.0
  fast_dev_run: False
  limit_train_batches: 1.0
  limit_val_batches: 1.0
  limit_test_batches: 1.0
  limit_predict_batches: 1.0
  accelerator: auto

# Validation settings; add new entries only in dedicated sections
val_settings:
  val_mode: repeated
  n_splits: 5
  start_idx: 0
  run_test: False
  seeds: [12345, 12346, 12347, 12348, 12349]
  aggregate_metrics: ['val/', 'test/']