# Global settings
global_settings:
  seed: 12345
  project: MIM
  experiment: SwinMIM-1ch-128_16_or # enc_mods_inp_mask
  root_dir: /projects/prjs1526/anyBrainer/experiments

# Logging settings
logging_settings:
  dev_mode: False
  worker_logs: False
  save_logs: True
  wandb_enable: True
  wandb_entity: petros-koutsouvelis-maastricht-university
  wandb_watch_enable: True
  wandb_watch_kwargs:
    log_freq: 5000
    log_graph: True
    log: 'all'

# Data settings
pl_datamodule_settings:
  name: MultimodalDataModule
  data_dir: /projects/prjs1526/data-pretraining/fomo-60k_clean
  data_handler_kwargs:
    data_format: GenericNifti
    exts: [".npy"]
  train_val_test_split: [0.95, 0.05, 0.0]
  num_workers: 12
  batch_size: 8
  extra_dataloader_kwargs:
    shuffle: True
    pin_memory: False
  train_transforms: 
    name: get_mim_transforms
    keys: ["ch1"]
    input_size: 128
    mask_size: 16
    mask_ratio_shared: [0.4, 0.6]
    mask_ratio_unique: [0.05, 0.15]
    val_mode: False
  val_transforms: 
    name: get_mim_transforms
    keys: ["ch1"]
    input_size: 128
    mask_size: 16
    mask_ratio_shared: [0.4, 0.6]
    mask_ratio_unique: [0.05, 0.15]
    val_mode: True
  test_transforms: 
    name: get_mim_transforms
    keys: ["ch1"]
    input_size: 128
    mask_size: 16
    mask_ratio_shared: [0.4, 0.6]
    mask_ratio_unique: [0.05, 0.15]
    val_mode: True
  extra_kwargs:
    modalities_per_ch: 
      - ["t1", "flair", "t2", "dwi", "unknown", "pd", "t2s", "swi"]
    distinct_modalities: False
    distinct_acquisitions: False

# Model
pl_module_settings:
  name: MultimodalMIMModel
  model_kwargs:
    name: Multimodal3DSwinMIMFPN
    in_channels: 1
    patch_size: 2
    depths: [2, 2, 6, 2]
    num_heads: [3, 6, 12, 24]
    window_size: 7
    embed_dim: 48
    use_v2: True
    extra_swin_kwargs:
      use_checkpoint: True
    use_vanilla_swin: False
    merge_mode: or
    inject_modality_tokens: False
    expected_modalities:
    fusion: none
    fpn_width: 32
  weights_init_settings:
    weights_init_fn: init_swin_v2
  optimizer_kwargs:
    name: AdamW
    auto_no_weight_decay: True
    lr: 0.0001
    weight_decay: 0.01
  lr_scheduler_kwargs:
    name: CosineAnnealingWithWarmup
    interval: step
    frequency: 1
    warmup_iters: 10000
    total_iters: 200000
    eta_min: 0.0001 # = base_lr --> no annealing
  loss_fn_kwargs:
    name: SmoothL1Loss
    beta: 1.0
  artifacts_settings:
    log_every_n_steps: 500
    log_max_n_items: 1

# Checkpoint settings
ckpt_settings:
  new_version: False
  model_checkpoint:
  save_every_n_steps: 25000
  save_top_k: -1
  save_last: True

# Callback settings (other than model checkpointing)
pl_callback_settings: []

# Trainer settings (pl.Trainer kwargs)
pl_trainer_settings:
  max_steps: 200000
  accumulate_grad_batches: 1
  gradient_clip_val: 1.0
  gradient_clip_algorithm: norm
  precision: bf16-mixed
  devices: 2
  strategy: ddp_find_unused_parameters_true
  accelerator: auto
  profiler: simple
  num_nodes: 1
  log_every_n_steps: 100
  val_check_interval: 1.0
  check_val_every_n_epoch: 1
  num_sanity_val_steps: 2
  overfit_batches: 0.0
  limit_train_batches: 1.0
  limit_val_batches: 1.0
  limit_test_batches: 1.0
  limit_predict_batches: 1.0
  fast_dev_run: False